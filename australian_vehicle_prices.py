# -*- coding: utf-8 -*-
"""Australian Vehicle Prices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ab-eZ_x2cfP232ajtVLUPnbCnomCoMLO
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.subplots import make_subplots
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

from sklearn.metrics import confusion_matrix

from sklearn.metrics import recall_score
from sklearn.metrics import precision_score

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('Australian Vehicle Prices.csv')

df.head()

df.replace(['POA', '-', '- / -'], np.nan, inplace=True)

df.info()

df.columns

df.shape

df['Price'] = pd.to_numeric(df['Price'], errors='coerce')
df['Kilometres'] = pd.to_numeric(df['Kilometres'], errors='coerce')

df.info()

df.nunique()



df.head()

df['FuelConsumption'] = df['FuelConsumption'].str.extract(r'(\d+\.\d+)').astype(float)
df['Doors'] = df['Doors'].str.extract(r'(\d+)').fillna(0).astype(int)
df['Seats'] = df['Seats'].str.extract(r'(\d+)').fillna(0).astype(int)
df['CylindersinEngine'] = df['CylindersinEngine'].str.extract(r'(\d+)').fillna(0).astype(int)
df['Engine'] = df['Engine'].str.extract(r'(\d+)').fillna(0).astype(int)

df.head()

df.isnull().sum()

per_missing_incoulmns=df.isnull().sum()/len(df)
per_missing_incoulmns

total_num_of_rows=len(df)
total_num_of_rows

# mask=df.isnull().any(axis=1)
# mask

# nam_of_rows_with_nan=mask.sum()
# nam_of_rows_with_nan

# (nam_of_rows_with_nan/total_num_of_rows)*100

# df=df[~mask]
# df

df.isnull().sum()

df.duplicated().sum()

df.head()

df.info()



# Apply unique to each column
unique_values = df.apply(lambda col: col.nunique())
print(unique_values)



numeric_cols = df.select_dtypes(include=[ 'float32','float64','int64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# Print the identified columns
print("Numeric columns:", numeric_cols)
print("Categorical columns:", categorical_cols)

for i in  ['UsedOrNew', 'Transmission', 'DriveType', 'FuelType', 'BodyType']:
    print(i,df[i].unique())



# Assuming you have a DataFrame 'df' and the output column is named 'target'
correlation = df.select_dtypes(include=[ 'float32','float64','int64']).corr()

# Display correlation values between each column and the output column
correlation_with_target = correlation['Price']

print(correlation_with_target)

# df=df.replace({'UsedOrNew':{'DEMO':1,'USED':2,'NEW':0}
#                                 ,
#                                 'Transmission':{'Automatic':0,'Manual':1}
#                                 })
# df.head()

# df=pd.get_dummies(df,columns=['Brand','DriveType', 'FuelType', 'BodyType','Brand','Year','Model','Car/Suv','Title','Location','ColourExtInt'])
# df.head()

import pandas as pd
from sklearn.preprocessing import LabelEncoder



# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Iterate through object columns and apply label encoding
for col in df.select_dtypes(include=['object']).columns:
    df[col] = label_encoder.fit_transform(df[col])

# Calculate the correlation matrix
correlation = df.corr()

# Display the correlation between encoded categorical columns and the price
correlation_with_price = correlation['Price']

print(correlation_with_price)

corr_matrix = df.corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 12))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=.5, vmin=-1, vmax=1)

# Add title and labels
plt.title('Correlation Heatmap')
plt.show()

df.drop(columns=['Brand','Model','Car/Suv','Title','Location','ColourExtInt','Seats'],inplace=True)

df.info()

df.isnull().sum()

df.dropna(subset=['Year','Price'], inplace=True)

df.isnull().sum()

# import pandas as pd
# import plotly.express as px
# import plotly.graph_objects as go
# from plotly.subplots import make_subplots


# # Get numerical columns
# numerical_cols = df.select_dtypes(include=['number']).columns

# # Create subplots, use numerical_cols for subplot titles
# fig = make_subplots(rows=1, cols=len(numerical_cols), subplot_titles=list(numerical_cols))

# # Add a box plot for each numerical column
# for idx, col in enumerate(numerical_cols, start=1):
#     fig.add_trace(go.Box(y=df[col], name=col), row=1, col=idx)

# # Update layout for better visualization
# fig.update_layout(height=500, width=300 * len(numerical_cols), title_text="Box Plots for All Numerical Columns")

# # Show the combined figure
# fig.show()

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots


# Get numerical columns
numerical_cols = df.select_dtypes(include=['number']).columns

for i in numerical_cols:
    fig = px.box(df, y=i)
    fig.show()

df['Kilometres']

df['Kilometres'].describe()

df['Kilometres'].median()

df['FuelConsumption']

df['FuelConsumption'].describe()

df['FuelConsumption'].median()

df[['Kilometres', 'FuelConsumption']] = df[['Kilometres', 'FuelConsumption']].fillna(df[['Kilometres', 'FuelConsumption']].median())

df.isnull().sum()

for i in numerical_cols:
    fig = px.box(df, y=i)
    fig.show()



fig1 = px.histogram(df, x="Price", nbins=50, title="Distribution of Car Prices")
fig1.show()

output=df['Price']
input=df.drop(columns=['Price'])

from sklearn.model_selection import train_test_split
X, X_test, y, y_test = train_test_split(input, output, test_size=0.30, random_state=0)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=0)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Convert the scaled arrays back into dataframes
X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_val = pd.DataFrame(X_val_scaled, columns=X_val.columns)
X_tes = pd.DataFrame(X_test_scaled, columns=X_test.columns)


print('X_train: ',X_train.shape)
print('Y_train: ',y_train.shape)
print('-----------------------------')
print('X_test: ',X_test.shape)
print('y_test: ',y_test.shape)
print('-----------------------------')
print('X_val: ',y_val.shape)
print('y_val: ',X_val.shape)





"""# Linear regression model"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

from sklearn.metrics import r2_score
#from sklearn.metrics import mean_squared_error

def eval_model(model, X_train, y_train, X_val, y_val):
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_val = model.predict(X_val)
    r2_train = r2_score(y_train, y_pred_train)
    r2_val = r2_score(y_val, y_pred_val)
    return r2_train, r2_val



from sklearn.linear_model import LinearRegression

linear_reg = LinearRegression()
eval_model(linear_reg, X_train, y_train, X_val, y_val)

eval_model(linear_reg, X_train, y_train, X_test, y_test)

import math
math.sqrt(0.37154725901417607)



"""# Polynomial Regression"""





# poly = PolynomialFeatures(degree=2, include_bias=True)
# x_train_trans = poly.fit_transform(X_train)
# x_test_trans = poly.transform(X_test)
# #include bias parameter
# lr = LinearRegression()
# lr.fit(x_train_trans, y_train)
# y_pred = lr.predict(x_test_trans)
# print(r2_score(y_test, y_pred))

# X_new = np.linspace(-3, 3, 200).reshape(200, 1)
# X_new_poly = poly.transform(X_new)
# y_new = lr.predict(X_new_poly)
# plt.plot(X_new, y_new, "r-", linewidth=2, label="Predictions")
# plt.plot(X_train, y_train, "b.",label='Training points')
# plt.plot(X_test, y_test, "g.",label='Testing points')
# plt.xlabel("X")
# plt.ylabel("y")
# plt.legend()
# plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# # Sample dataset for illustration (replace with your own dataset)
# X_train = np.random.rand(100, 1) * 6 - 3  # Example: random data between -3 and 3
# y_train = 2 * (X_train ** 2) + np.random.randn(100, 1)  # Quadratic relationship

# Create polynomial features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)

# Fit a linear regression model
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train_poly, y_train)

# Create new data for prediction

# Transform new data using the same polynomial features
X_new_poly = poly.transform(X_train)

# Make predictions
y_new = lr.predict(X_new_poly)







poly = PolynomialFeatures(degree=2, include_bias=True)
x_train_trans = poly.fit_transform(X_train)
x_test_trans = poly.transform(X_test)
#include bias parameter
lr = LinearRegression()
lr.fit(x_train_trans, y_train)
y_pred = lr.predict(x_test_trans)
print(r2_score(y_test, y_pred))

"""# knn"""

from sklearn.neighbors import KNeighborsClassifier

modelKNN = KNeighborsClassifier(n_neighbors=2)

outKNNacc = modelKNN.fit(X_train, y_train)

y_pred_train = modelKNN.predict(X_train)
y_pred_val = modelKNN.predict(X_val)

print("Training Result: ", accuracy_score(y_train, y_pred_train))
print("Validation Result: ",accuracy_score(y_val, y_pred_val))

print("Testing Result: ", accuracy_score(y_test, modelKNN.predict(X_test)))

"""# using cv"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.model_selection import train_test_split, cross_val_score
import warnings
warnings.filterwarnings(action='ignore')

results = {}

regression_algorithms = [
    LinearRegression(),
    Ridge(),
    Lasso(),
    ElasticNet(),
    DecisionTreeRegressor(),
    RandomForestRegressor(),
    GradientBoostingRegressor(),
    SVR(),
    KNeighborsRegressor(),
    MLPRegressor(),
    AdaBoostRegressor(),
    BaggingRegressor(),
    ExtraTreesRegressor()
]

for model in regression_algorithms:
    model_name = model.__class__.__name__
    scores = cross_val_score(model, X_train, y_train, cv=3)
    results[model_name] = scores

for model_name, scores in results.items():
    print(model_name,"------------------------->", scores, sum(scores)/len(scores))

model_names = list(results.keys())
average_scores = [np.mean(scores) for scores in results.values()]

plt.figure(figsize=(10, 6))
plt.barh(model_names, average_scores, color='skyblue')
plt.xlabel('Average Cross-Validation Score')
plt.title('Comparison of Regression Algorithms')
plt.gca().invert_yaxis()
plt.show()

results.values()

average_scores

all_scores={}
for model_name, value in results.items():
    all_scores[model_name]= sum(value)/len(value)

best_model_name = max(all_scores, key=all_scores.get)

import pickle

max_score_model = max(all_scores, key=all_scores.get)
max_score_value = all_scores[max_score_model]

print(f"The model with the largest score is '{max_score_model}' with an average score of {max_score_value:.4f}.")

best_model = regression_algorithms[[model.__class__.__name__ for model in regression_algorithms].index(max_score_model)]
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

with open(f"{max_score_model}.pkl", 'wb') as file:
    pickle.dump(best_model, file)

print(f"Saved the model as '{max_score_model}.pkl'.")

from sklearn.ensemble import RandomForestRegressor
import pickle

# Assume model is your trained RandomForestRegressor
model = RandomForestRegressor()
# Train your model here
model.fit(X_train, y_train)

# Save the model
with open('vehicle_price_model.pkl', 'wb') as file:
    pickle.dump(model, file)

from joblib import dump
# Save the model
dump(model, 'vehicle_price_model.joblib')

df.columns

df.info()

input.columns

